{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e00a3c2-cf68-43fa-b956-83453006e75b",
   "metadata": {},
   "source": [
    "# Cross-Entropy for Taxi-v3\n",
    "После лекции 1 и практического занятия 1 требуется выполнить три домашних задания:\n",
    "\n",
    "1. Пользуясь алгоритмом Кросс-Энтропии обучить агента решать задачу Taxi-v3 из Gym. Исследовать гиперпараметры алгоритма и выбрать лучшие.\n",
    "2. Реализовать алгоритм Кросс-Энтропии с двумя типами сглаживания,  указанными в лекции 1. При выбранных в пункте 1 гиперпараметров сравнить их результаты с результатами алгоритма без сглаживания.\n",
    "3. Реализовать модификацию алгоритм Кросс-Энтропии для стохастических сред, указанную в лекции 1. Сравнить ее результат с алгоритмами из пунктов 1 и 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4521c740-da70-40b8-afc0-c4669a388b99",
   "metadata": {},
   "source": [
    "## Импорт библиотек "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "17987af5-3dfa-494d-9c65-317a176b47d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe'\n",
    "\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_parallel_coordinate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c15780-c089-4a95-82d9-8b1b9eb6d544",
   "metadata": {},
   "source": [
    "## Класс `CrossEntropyAgent`\n",
    "Метод Кросс-Энтропии состоит из 2 шагов:\n",
    "1. **Оценка стратегии** - поиск $\\mathbb{E}_\\pi[G]$\n",
    "\n",
    "Пусть $\\pi_0$ -- это исходная стратегия, $N$ -- это число итераций. На каждой итерации выбираем $K$ траекторий и выбираем *\"элитные\" траектории*: такие траектории, чья награда превышает $q$-квантиль. \n",
    "\n",
    "2. **Улучшение стратегии** - поиск $\\pi' \\geq \\pi: \\mathbb{E}_\\pi[G] \\geq\\mathbb{E}_{\\pi'}[G]$\n",
    "\n",
    "Выбрав \"элитные\" траектории, обновляем стратегию следующим образом:$$ \\pi_{n+1}(a|s) = \\frac{\\text{число пар $(a|s)$ в \"элитных\" траекториях}}{\\text{число состояний $s$ в \"элитных\" траекториях}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "54166c9b-8a37-4469-9571-8c681a9ba2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyAgent:\n",
    "    def __init__(self, state_n=500, action_n=6):\n",
    "        self.state_n = state_n\n",
    "        self.action_n = action_n\n",
    "        self.model = np.ones((self.state_n, self.action_n)) / self.action_n\n",
    "\n",
    "    def get_action(self, state):\n",
    "        action = np.random.choice(np.arange(self.action_n), p=self.model[state])\n",
    "        return int(action)\n",
    "\n",
    "    def fit(self, elite_trajectories):\n",
    "        new_model = np.zeros((self.state_n, self.action_n))\n",
    "        for trajectory in elite_trajectories:\n",
    "            for state, action in zip(trajectory[\"states\"], trajectory[\"actions\"]):\n",
    "                new_model[state][action] += 1\n",
    "\n",
    "        for state in range(self.state_n):\n",
    "            if np.sum(new_model[state]) > 0:\n",
    "                new_model[state] /= np.sum(new_model[state])\n",
    "            else:\n",
    "                new_model[state] = self.model[state].copy()\n",
    "\n",
    "        self.model = new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2350a2-052e-4739-8a67-18ae72c6ef93",
   "metadata": {},
   "source": [
    "## Методы для получения состояния и траекторий "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "83d6de6a-212b-46b7-b8b2-6568f7ad224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(obs):\n",
    "    return obs\n",
    "\n",
    "\n",
    "def get_trajectory(env, agent, max_len=1000, visualize=False):\n",
    "    trajectory = {\"states\": [], \"actions\": [], \"rewards\": []}\n",
    "\n",
    "    obs = env.reset()\n",
    "    state = get_state(obs)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        trajectory[\"states\"].append(state)\n",
    "\n",
    "        action = agent.get_action(state)\n",
    "        trajectory[\"actions\"].append(action)\n",
    "\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        trajectory[\"rewards\"].append(reward)\n",
    "\n",
    "        state = get_state(obs)\n",
    "\n",
    "        if visualize:\n",
    "            time.sleep(0.5)\n",
    "            env.render()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a50cd1-16b3-463c-b5c5-12db2458cc54",
   "metadata": {},
   "source": [
    "## Класс `Environment` и метод для обучения агента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9325399a-d31c-47e9-a45b-0470684e5d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, env_name='Taxi-v3', q_param=0.9, iteration_n=100, trajectory_n=50):\n",
    "        self.env_name = env_name\n",
    "        self.q_param = q_param\n",
    "        self.iteration_n = iteration_n\n",
    "        self.iteration_range = np.array(range(self.iteration_n))\n",
    "        self.trajectory_n = trajectory_n\n",
    "\n",
    "def train_agent(environment, agent, verbose=True):\n",
    "    env = gym.make(environment.env_name)\n",
    "    env.reset()\n",
    "    mean_total_reward_per_iteration = np.zeros(environment.iteration_n)\n",
    "    \n",
    "    for iteration in environment.iteration_range:\n",
    "    \n",
    "        #policy evaluation\n",
    "        trajectories = [get_trajectory(env, agent) for _ in range(environment.trajectory_n)]\n",
    "        total_rewards = [np.sum(trajectory['rewards']) for trajectory in trajectories]\n",
    "        mean_total_reward_per_iteration[iteration] = np.mean(total_rewards)\n",
    "        if verbose:\n",
    "            print('iteration:', iteration, 'mean total reward:', mean_total_reward_per_iteration[iteration])\n",
    "    \n",
    "        #policy improvement\n",
    "        quantile = np.quantile(total_rewards, environment.q_param)\n",
    "        elite_trajectories = []\n",
    "        for trajectory in trajectories:\n",
    "            total_reward = np.sum(trajectory['rewards'])\n",
    "            if total_reward > quantile:\n",
    "                elite_trajectories.append(trajectory)\n",
    "    \n",
    "        agent.fit(elite_trajectories)\n",
    "\n",
    "    return mean_total_reward_per_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35793311-1f17-4592-9f4a-95b01300bbb1",
   "metadata": {},
   "source": [
    "## 1. Обучим агента и подберем гиперпараметры"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7d8f85-1142-46c7-905f-ad3e83618564",
   "metadata": {},
   "source": [
    "### Создадим игру и обучим агента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ea74c719-de97-4e62-a3ab-ab5b0f6af0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Environment()\n",
    "agent = CrossEntropyAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bf56a691-8960-4723-b3c6-98a52cf70bcf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 mean total reward: -787.0\n",
      "iteration: 1 mean total reward: -710.6\n",
      "iteration: 2 mean total reward: -668.54\n",
      "iteration: 3 mean total reward: -669.26\n",
      "iteration: 4 mean total reward: -551.04\n",
      "iteration: 5 mean total reward: -503.98\n",
      "iteration: 6 mean total reward: -546.72\n",
      "iteration: 7 mean total reward: -498.4\n",
      "iteration: 8 mean total reward: -417.78\n",
      "iteration: 9 mean total reward: -399.26\n",
      "iteration: 10 mean total reward: -485.5\n",
      "iteration: 11 mean total reward: -480.62\n",
      "iteration: 12 mean total reward: -464.7\n",
      "iteration: 13 mean total reward: -445.88\n",
      "iteration: 14 mean total reward: -514.78\n",
      "iteration: 15 mean total reward: -510.1\n",
      "iteration: 16 mean total reward: -504.5\n",
      "iteration: 17 mean total reward: -433.28\n",
      "iteration: 18 mean total reward: -456.0\n",
      "iteration: 19 mean total reward: -485.1\n",
      "iteration: 20 mean total reward: -412.82\n",
      "iteration: 21 mean total reward: -520.34\n",
      "iteration: 22 mean total reward: -395.9\n",
      "iteration: 23 mean total reward: -478.86\n",
      "iteration: 24 mean total reward: -431.68\n",
      "iteration: 25 mean total reward: -449.78\n",
      "iteration: 26 mean total reward: -527.64\n",
      "iteration: 27 mean total reward: -550.9\n",
      "iteration: 28 mean total reward: -450.62\n",
      "iteration: 29 mean total reward: -411.84\n",
      "iteration: 30 mean total reward: -473.14\n",
      "iteration: 31 mean total reward: -351.56\n",
      "iteration: 32 mean total reward: -472.98\n",
      "iteration: 33 mean total reward: -469.52\n",
      "iteration: 34 mean total reward: -433.96\n",
      "iteration: 35 mean total reward: -423.94\n",
      "iteration: 36 mean total reward: -454.46\n",
      "iteration: 37 mean total reward: -554.46\n",
      "iteration: 38 mean total reward: -348.7\n",
      "iteration: 39 mean total reward: -419.64\n",
      "iteration: 40 mean total reward: -457.42\n",
      "iteration: 41 mean total reward: -392.22\n",
      "iteration: 42 mean total reward: -441.8\n",
      "iteration: 43 mean total reward: -519.24\n",
      "iteration: 44 mean total reward: -432.58\n",
      "iteration: 45 mean total reward: -570.72\n",
      "iteration: 46 mean total reward: -516.8\n",
      "iteration: 47 mean total reward: -403.8\n",
      "iteration: 48 mean total reward: -406.7\n",
      "iteration: 49 mean total reward: -504.46\n",
      "iteration: 50 mean total reward: -360.38\n",
      "iteration: 51 mean total reward: -460.5\n",
      "iteration: 52 mean total reward: -488.46\n",
      "iteration: 53 mean total reward: -468.56\n",
      "iteration: 54 mean total reward: -406.96\n",
      "iteration: 55 mean total reward: -459.1\n",
      "iteration: 56 mean total reward: -520.42\n",
      "iteration: 57 mean total reward: -490.8\n",
      "iteration: 58 mean total reward: -428.36\n",
      "iteration: 59 mean total reward: -475.46\n",
      "iteration: 60 mean total reward: -421.02\n",
      "iteration: 61 mean total reward: -437.78\n",
      "iteration: 62 mean total reward: -549.34\n",
      "iteration: 63 mean total reward: -495.72\n",
      "iteration: 64 mean total reward: -389.2\n",
      "iteration: 65 mean total reward: -409.04\n",
      "iteration: 66 mean total reward: -378.52\n",
      "iteration: 67 mean total reward: -396.24\n",
      "iteration: 68 mean total reward: -415.92\n",
      "iteration: 69 mean total reward: -461.98\n",
      "iteration: 70 mean total reward: -405.74\n",
      "iteration: 71 mean total reward: -476.0\n",
      "iteration: 72 mean total reward: -438.4\n",
      "iteration: 73 mean total reward: -524.26\n",
      "iteration: 74 mean total reward: -462.48\n",
      "iteration: 75 mean total reward: -400.14\n",
      "iteration: 76 mean total reward: -463.3\n",
      "iteration: 77 mean total reward: -437.7\n",
      "iteration: 78 mean total reward: -484.32\n",
      "iteration: 79 mean total reward: -492.26\n",
      "iteration: 80 mean total reward: -455.88\n",
      "iteration: 81 mean total reward: -412.56\n",
      "iteration: 82 mean total reward: -370.12\n",
      "iteration: 83 mean total reward: -442.82\n",
      "iteration: 84 mean total reward: -426.48\n",
      "iteration: 85 mean total reward: -473.28\n",
      "iteration: 86 mean total reward: -383.12\n",
      "iteration: 87 mean total reward: -354.96\n",
      "iteration: 88 mean total reward: -491.44\n",
      "iteration: 89 mean total reward: -394.78\n",
      "iteration: 90 mean total reward: -476.82\n",
      "iteration: 91 mean total reward: -466.62\n",
      "iteration: 92 mean total reward: -429.92\n",
      "iteration: 93 mean total reward: -445.24\n",
      "iteration: 94 mean total reward: -388.54\n",
      "iteration: 95 mean total reward: -383.38\n",
      "iteration: 96 mean total reward: -377.14\n",
      "iteration: 97 mean total reward: -516.06\n",
      "iteration: 98 mean total reward: -401.96\n",
      "iteration: 99 mean total reward: -448.42\n",
      "total reward: -388\n",
      "model:\n",
      "[[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         0.         0.         0.         1.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " ...\n",
      " [0.21875    0.15625    0.125      0.21875    0.15625    0.125     ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         0.         0.         1.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "mean_total_reward_per_iteration = train_agent(environment, agent)\n",
    "\n",
    "trajectory = get_trajectory(env, agent, max_len=100, visualize=False)\n",
    "env.close()\n",
    "print('total reward:', sum(trajectory['rewards']))\n",
    "print('model:')\n",
    "print(agent.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7f5a155f-473d-4cfe-b951-09b64873a217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_77.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=environment.iteration_range, y=mean_total_reward_per_iteration,\n",
    "                    mode='lines+markers',\n",
    "                    name='CE_agent_q_0.9_traj_50'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Cross-Entropy Agent: q=0.9, trajectory_n=50\",\n",
    "    xaxis_title=\"Iteration\",\n",
    "    yaxis_title=\"Mean Total Reward\",\n",
    "    legend_title=\"Legend Title\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ca06ae-ea14-40d2-a629-d72fefa592ce",
   "metadata": {},
   "source": [
    "### Подбор гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4a2fb48d-809e-4bd8-8510-7cc7cea6cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    q_param = trial.suggest_float('q_param', 0, 1, log=False)\n",
    "    trajectory_n = trial.suggest_int('trajectory_n', 10, 500)\n",
    "\n",
    "    agent = CrossEntropyAgent()\n",
    "    environment = Environment(q_param=q_param, trajectory_n=trajectory_n)\n",
    "    \n",
    "    mean_reward = train_agent(environment, agent, verbose=False)\n",
    "    \n",
    "    return mean_reward[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "52fa357b-8f80-43df-9912-b89cc227076f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-12 14:49:40,700] A new study created in memory with name: no-name-e1a07723-eb74-4d8a-981e-368462d0ff88\n",
      "[I 2023-10-12 14:50:02,504] Trial 0 finished with value: 6.8995098039215685 and parameters: {'q_param': 0.43512845271204725, 'trajectory_n': 408}. Best is trial 0 with value: 6.8995098039215685.\n",
      "[I 2023-10-12 14:50:28,529] Trial 1 finished with value: -270.0357142857143 and parameters: {'q_param': 0.9003996761756318, 'trajectory_n': 196}. Best is trial 0 with value: 6.8995098039215685.\n",
      "[I 2023-10-12 14:52:45,025] Trial 2 finished with value: -168.5935960591133 and parameters: {'q_param': 0.04370846597180911, 'trajectory_n': 406}. Best is trial 0 with value: 6.8995098039215685.\n",
      "[I 2023-10-12 14:53:13,720] Trial 3 finished with value: -461.0378787878788 and parameters: {'q_param': 0.947143760805957, 'trajectory_n': 132}. Best is trial 0 with value: 6.8995098039215685.\n",
      "[I 2023-10-12 14:53:55,903] Trial 4 finished with value: -225.57926829268294 and parameters: {'q_param': 0.5623784383335293, 'trajectory_n': 164}. Best is trial 0 with value: 6.8995098039215685.\n",
      "[I 2023-10-12 14:54:34,426] Trial 5 finished with value: -43.391176470588235 and parameters: {'q_param': 0.48771257693753045, 'trajectory_n': 340}. Best is trial 0 with value: 6.8995098039215685.\n",
      "[I 2023-10-12 14:55:24,985] Trial 6 finished with value: -68.33114754098361 and parameters: {'q_param': 0.22802741348502453, 'trajectory_n': 305}. Best is trial 0 with value: 6.8995098039215685.\n",
      "[I 2023-10-12 14:56:27,418] Trial 7 finished with value: -209.40106951871658 and parameters: {'q_param': 0.02161639709470453, 'trajectory_n': 187}. Best is trial 0 with value: 6.8995098039215685.\n",
      "[I 2023-10-12 14:56:56,297] Trial 8 finished with value: -71.06122448979592 and parameters: {'q_param': 0.5860321588938535, 'trajectory_n': 294}. Best is trial 0 with value: 6.8995098039215685.\n",
      "[I 2023-10-12 14:57:20,217] Trial 9 finished with value: 5.960416666666666 and parameters: {'q_param': 0.5312358672812959, 'trajectory_n': 480}. Best is trial 0 with value: 6.8995098039215685.\n",
      "[I 2023-10-12 14:57:39,224] Trial 10 finished with value: -267.35483870967744 and parameters: {'q_param': 0.3505433266460629, 'trajectory_n': 62}. Best is trial 0 with value: 6.8995098039215685.\n",
      "[I 2023-10-12 14:58:13,968] Trial 11 finished with value: -63.882 and parameters: {'q_param': 0.6755378995938255, 'trajectory_n': 500}. Best is trial 0 with value: 6.8995098039215685.\n",
      "[I 2023-10-12 14:59:18,672] Trial 12 finished with value: -53.4 and parameters: {'q_param': 0.3866345831567788, 'trajectory_n': 475}. Best is trial 0 with value: 6.8995098039215685.\n",
      "[I 2023-10-12 14:59:35,993] Trial 13 finished with value: 1.1166253101736974 and parameters: {'q_param': 0.7108326955344553, 'trajectory_n': 403}. Best is trial 0 with value: 6.8995098039215685.\n",
      "[I 2023-10-12 14:59:57,523] Trial 14 finished with value: 6.860294117647059 and parameters: {'q_param': 0.4137421351750722, 'trajectory_n': 408}. Best is trial 0 with value: 6.8995098039215685.\n",
      "[I 2023-10-12 15:01:18,713] Trial 15 finished with value: -77.28009828009829 and parameters: {'q_param': 0.2601972116393683, 'trajectory_n': 407}. Best is trial 0 with value: 6.8995098039215685.\n",
      "[I 2023-10-12 15:01:49,516] Trial 16 finished with value: -27.8135593220339 and parameters: {'q_param': 0.41410490254106286, 'trajectory_n': 354}. Best is trial 0 with value: 6.8995098039215685.\n",
      "[I 2023-10-12 15:02:34,752] Trial 17 finished with value: -30.732510288065843 and parameters: {'q_param': 0.24947995780721544, 'trajectory_n': 243}. Best is trial 0 with value: 6.8995098039215685.\n",
      "[I 2023-10-12 15:02:54,246] Trial 18 finished with value: 0.22095671981776766 and parameters: {'q_param': 0.6946110375342643, 'trajectory_n': 439}. Best is trial 0 with value: 6.8995098039215685.\n",
      "[I 2023-10-12 15:03:10,942] Trial 19 finished with value: 6.481586402266289 and parameters: {'q_param': 0.45516981943852497, 'trajectory_n': 353}. Best is trial 0 with value: 6.8995098039215685.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "Value:  6.8995098039215685\n",
      "Params: \n",
      "  q_param: 0.43512845271204725\n",
      "  trajectory_n: 408\n"
     ]
    }
   ],
   "source": [
    "sampler = TPESampler()#n_startup_trials=10\n",
    "pruner = MedianPruner()\n",
    "\n",
    "study = optuna.create_study(sampler=sampler, pruner=pruner, direction='maximize')  # For reward maximization\n",
    "\n",
    "# Run the hyperparameter optimization\n",
    "study.optimize(objective, n_trials=20)  # You can adjust n_trials\n",
    "\n",
    "# Print the best hyperparameters and result\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"Value: \", trial.value)\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "825c89e8-6efc-4fe0-a32f-249a9be2cf49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_140.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plot_optimization_history(study, target_name='Reward')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "57923d9a-663a-4df3-b941-70135aa1749d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_141.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plot_parallel_coordinate(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87d6538-9d97-4db4-9503-6e40cb51e936",
   "metadata": {},
   "source": [
    "Обучим агента на лучших параметрах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "95a300b8-c586-49fd-9297-6909d8a72a49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 mean total reward: -759.25\n",
      "iteration: 1 mean total reward: -694.3872549019608\n",
      "iteration: 2 mean total reward: -608.7720588235294\n",
      "iteration: 3 mean total reward: -531.1617647058823\n",
      "iteration: 4 mean total reward: -447.3014705882353\n",
      "iteration: 5 mean total reward: -354.97549019607845\n",
      "iteration: 6 mean total reward: -270.22058823529414\n",
      "iteration: 7 mean total reward: -170.9436274509804\n",
      "iteration: 8 mean total reward: -108.22058823529412\n",
      "iteration: 9 mean total reward: -70.36029411764706\n",
      "iteration: 10 mean total reward: -43.26960784313726\n",
      "iteration: 11 mean total reward: -23.61519607843137\n",
      "iteration: 12 mean total reward: -10.034313725490197\n",
      "iteration: 13 mean total reward: -1.5416666666666667\n",
      "iteration: 14 mean total reward: 2.4779411764705883\n",
      "iteration: 15 mean total reward: 2.877450980392157\n",
      "iteration: 16 mean total reward: 4.509803921568627\n",
      "iteration: 17 mean total reward: 4.7818627450980395\n",
      "iteration: 18 mean total reward: 5.703431372549019\n",
      "iteration: 19 mean total reward: 5.897058823529412\n",
      "iteration: 20 mean total reward: 6.620098039215686\n",
      "iteration: 21 mean total reward: 5.713235294117647\n",
      "iteration: 22 mean total reward: 6.404411764705882\n",
      "iteration: 23 mean total reward: 5.8088235294117645\n",
      "iteration: 24 mean total reward: 5.730392156862745\n",
      "iteration: 25 mean total reward: 6.009803921568627\n",
      "iteration: 26 mean total reward: 5.732843137254902\n",
      "iteration: 27 mean total reward: 5.840686274509804\n",
      "iteration: 28 mean total reward: 7.051470588235294\n",
      "iteration: 29 mean total reward: 6.357843137254902\n",
      "iteration: 30 mean total reward: 6.865196078431373\n",
      "iteration: 31 mean total reward: 6.299019607843137\n",
      "iteration: 32 mean total reward: 6.254901960784314\n",
      "iteration: 33 mean total reward: 6.0661764705882355\n",
      "iteration: 34 mean total reward: 5.590686274509804\n",
      "iteration: 35 mean total reward: 6.598039215686274\n",
      "iteration: 36 mean total reward: 5.6004901960784315\n",
      "iteration: 37 mean total reward: 6.012254901960785\n",
      "iteration: 38 mean total reward: 6.1838235294117645\n",
      "iteration: 39 mean total reward: 6.328431372549019\n",
      "iteration: 40 mean total reward: 5.705882352941177\n",
      "iteration: 41 mean total reward: 6.732843137254902\n",
      "iteration: 42 mean total reward: 6.627450980392157\n",
      "iteration: 43 mean total reward: 5.830882352941177\n",
      "iteration: 44 mean total reward: 6.284313725490196\n",
      "iteration: 45 mean total reward: 6.5318627450980395\n",
      "iteration: 46 mean total reward: 6.227941176470588\n",
      "iteration: 47 mean total reward: 6.504901960784314\n",
      "iteration: 48 mean total reward: 6.568627450980392\n",
      "iteration: 49 mean total reward: 7.014705882352941\n",
      "iteration: 50 mean total reward: 6.384803921568627\n",
      "iteration: 51 mean total reward: 6.536764705882353\n",
      "iteration: 52 mean total reward: 5.612745098039215\n",
      "iteration: 53 mean total reward: 5.620098039215686\n",
      "iteration: 54 mean total reward: 5.517156862745098\n",
      "iteration: 55 mean total reward: 6.198529411764706\n",
      "iteration: 56 mean total reward: 6.830882352941177\n",
      "iteration: 57 mean total reward: 6.551470588235294\n",
      "iteration: 58 mean total reward: 6.2181372549019605\n",
      "iteration: 59 mean total reward: 6.997549019607843\n",
      "iteration: 60 mean total reward: 6.6495098039215685\n",
      "iteration: 61 mean total reward: 5.823529411764706\n",
      "iteration: 62 mean total reward: 6.946078431372549\n",
      "iteration: 63 mean total reward: 5.990196078431373\n",
      "iteration: 64 mean total reward: 6.151960784313726\n",
      "iteration: 65 mean total reward: 6.754901960784314\n",
      "iteration: 66 mean total reward: 5.838235294117647\n",
      "iteration: 67 mean total reward: 6.254901960784314\n",
      "iteration: 68 mean total reward: 6.605392156862745\n",
      "iteration: 69 mean total reward: 6.161764705882353\n",
      "iteration: 70 mean total reward: 6.208333333333333\n",
      "iteration: 71 mean total reward: 6.142156862745098\n",
      "iteration: 72 mean total reward: 5.892156862745098\n",
      "iteration: 73 mean total reward: 6.098039215686274\n",
      "iteration: 74 mean total reward: 6.732843137254902\n",
      "iteration: 75 mean total reward: 6.921568627450981\n",
      "iteration: 76 mean total reward: 6.6911764705882355\n",
      "iteration: 77 mean total reward: 6.53921568627451\n",
      "iteration: 78 mean total reward: 6.754901960784314\n",
      "iteration: 79 mean total reward: 6.9754901960784315\n",
      "iteration: 80 mean total reward: 5.3504901960784315\n",
      "iteration: 81 mean total reward: 5.448529411764706\n",
      "iteration: 82 mean total reward: 6.9754901960784315\n",
      "iteration: 83 mean total reward: 6.120098039215686\n",
      "iteration: 84 mean total reward: 6.610294117647059\n",
      "iteration: 85 mean total reward: 6.634803921568627\n",
      "iteration: 86 mean total reward: 6.644607843137255\n",
      "iteration: 87 mean total reward: 6.855392156862745\n",
      "iteration: 88 mean total reward: 6.377450980392157\n",
      "iteration: 89 mean total reward: 6.681372549019608\n",
      "iteration: 90 mean total reward: 6.455882352941177\n",
      "iteration: 91 mean total reward: 6.698529411764706\n",
      "iteration: 92 mean total reward: 6.083333333333333\n",
      "iteration: 93 mean total reward: 5.882352941176471\n",
      "iteration: 94 mean total reward: 6.752450980392157\n",
      "iteration: 95 mean total reward: 6.91421568627451\n",
      "iteration: 96 mean total reward: 5.7818627450980395\n",
      "iteration: 97 mean total reward: 6.355392156862745\n",
      "iteration: 98 mean total reward: 5.928921568627451\n",
      "iteration: 99 mean total reward: 6.490196078431373\n",
      "total reward: 1\n",
      "model:\n",
      "[[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         0.         0.         0.         1.         0.        ]\n",
      " [0.         0.         0.         0.         1.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "environment = Environment(**trial.params)\n",
    "agent = CrossEntropyAgent()\n",
    "\n",
    "mean_total_reward_per_iteration = train_agent(environment, agent)\n",
    "\n",
    "trajectory = get_trajectory(env, agent, max_len=100, visualize=False)\n",
    "env.close()\n",
    "print('total reward:', sum(trajectory['rewards']))\n",
    "print('model:')\n",
    "print(agent.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f8dd785a-7e4c-495a-815b-71480fb18bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_143.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=environment.iteration_range, y=mean_total_reward_per_iteration,\n",
    "                    mode='lines+markers',\n",
    "                    name='CE_agent_q_0.9_traj_50'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Cross-Entropy Agent: q={environment.q_param:1f}, trajectory_n={environment.trajectory_n}\",\n",
    "    xaxis_title=\"Iteration\",\n",
    "    yaxis_title=\"Mean Total Reward\",\n",
    "    legend_title=\"Legend Title\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2d8564-ddcd-4d4b-8251-c2ed18dab387",
   "metadata": {},
   "source": [
    "## 2. Сглаживания \n",
    "Обновление стратегии сильно зависит от случайности (начиная от случайной инициализации начальной стратегии, заканчивая ситуациями, когда в одном и том же состоянии в элитных траекториях выбирались неоптимальные действия), для того, чтобы с этим бороться, применяют техники сглаживания:\n",
    "1. Сглаживание по Лапласу $$  \\pi_{n+1}(a|s) = \\frac{|(a|s)\\in T_n| + \\lambda}{|s\\in T_n|+\\lambda}, \\text{ for }\\lambda > 0  $$\n",
    "2. Сглаживание стратегии $$  \\pi_{n+1}(a|s) = \\lambda\\pi_{n+1}(a|s) + (1-\\lambda)\\pi_n(a|s), \\text{ for }\\lambda >0  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c2cc4731-c76b-4896-8b53-9a76724c72cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyAgent:\n",
    "    def __init__(\n",
    "        self, state_n=500, action_n=6, smoothing=None, laplace_lambda=0.5, policy_lambda=0.5\n",
    "    ):\n",
    "        self.state_n = state_n\n",
    "        self.action_n = action_n\n",
    "        self.smoothing = smoothing\n",
    "        self.laplace_lambda = laplace_lambda\n",
    "        self.policy_lambda = policy_lambda\n",
    "        self.model = np.ones((self.state_n, self.action_n)) / self.action_n\n",
    "\n",
    "    def get_action(self, state):\n",
    "        action = np.random.choice(np.arange(self.action_n), p=self.model[state])\n",
    "        return int(action)\n",
    "\n",
    "    def fit(self, elite_trajectories):\n",
    "        new_model = np.zeros((self.state_n, self.action_n))\n",
    "        for trajectory in elite_trajectories:\n",
    "            for state, action in zip(trajectory[\"states\"], trajectory[\"actions\"]):\n",
    "                new_model[state][action] += 1\n",
    "\n",
    "        for state in range(self.state_n):\n",
    "            if np.sum(new_model[state]) > 0:\n",
    "                if self.smoothing == \"laplace\" or self.smoothing == \"both\":\n",
    "                    new_model[state] += self.laplace_lambda\n",
    "                    new_model[state] /= np.sum(new_model[state])\n",
    "                else:\n",
    "                    new_model[state] /= np.sum(new_model[state])\n",
    "            else:\n",
    "                new_model[state] = self.model[state].copy()\n",
    "\n",
    "        if self.smoothing == \"policy\" or self.smoothing == \"both\":\n",
    "            new_model = (\n",
    "                self.policy_lambda * new_model + (1 - self.policy_lambda) * self.model\n",
    "            )\n",
    "\n",
    "        self.model = new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1385c9f4-de66-4c40-833f-8895644bae10",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------\n",
      "0: Smoothing = None\n",
      "iteration: 0 mean total reward: -769.78\n",
      "iteration: 1 mean total reward: -734.88\n",
      "iteration: 2 mean total reward: -576.22\n",
      "iteration: 3 mean total reward: -552.4\n",
      "iteration: 4 mean total reward: -522.34\n",
      "iteration: 5 mean total reward: -619.52\n",
      "iteration: 6 mean total reward: -619.18\n",
      "iteration: 7 mean total reward: -634.58\n",
      "iteration: 8 mean total reward: -553.64\n",
      "iteration: 9 mean total reward: -612.72\n",
      "iteration: 10 mean total reward: -664.06\n",
      "iteration: 11 mean total reward: -555.6\n",
      "iteration: 12 mean total reward: -651.4\n",
      "iteration: 13 mean total reward: -553.0\n",
      "iteration: 14 mean total reward: -579.1\n",
      "iteration: 15 mean total reward: -646.48\n",
      "iteration: 16 mean total reward: -629.12\n",
      "iteration: 17 mean total reward: -626.08\n",
      "iteration: 18 mean total reward: -513.02\n",
      "iteration: 19 mean total reward: -521.08\n",
      "iteration: 20 mean total reward: -566.9\n",
      "iteration: 21 mean total reward: -535.36\n",
      "iteration: 22 mean total reward: -578.0\n",
      "iteration: 23 mean total reward: -529.9\n",
      "iteration: 24 mean total reward: -517.82\n",
      "iteration: 25 mean total reward: -502.76\n",
      "iteration: 26 mean total reward: -557.08\n",
      "iteration: 27 mean total reward: -497.9\n",
      "iteration: 28 mean total reward: -657.22\n",
      "iteration: 29 mean total reward: -468.62\n",
      "iteration: 30 mean total reward: -447.54\n",
      "iteration: 31 mean total reward: -546.74\n",
      "iteration: 32 mean total reward: -434.86\n",
      "iteration: 33 mean total reward: -484.22\n",
      "iteration: 34 mean total reward: -498.48\n",
      "iteration: 35 mean total reward: -568.26\n",
      "iteration: 36 mean total reward: -575.1\n",
      "iteration: 37 mean total reward: -518.16\n",
      "iteration: 38 mean total reward: -522.42\n",
      "iteration: 39 mean total reward: -512.38\n",
      "iteration: 40 mean total reward: -391.28\n",
      "iteration: 41 mean total reward: -593.26\n",
      "iteration: 42 mean total reward: -584.78\n",
      "iteration: 43 mean total reward: -458.76\n",
      "iteration: 44 mean total reward: -421.48\n",
      "iteration: 45 mean total reward: -544.44\n",
      "iteration: 46 mean total reward: -584.08\n",
      "iteration: 47 mean total reward: -455.9\n",
      "iteration: 48 mean total reward: -534.08\n",
      "iteration: 49 mean total reward: -523.06\n",
      "iteration: 50 mean total reward: -569.14\n",
      "iteration: 51 mean total reward: -482.58\n",
      "iteration: 52 mean total reward: -591.52\n",
      "iteration: 53 mean total reward: -556.66\n",
      "iteration: 54 mean total reward: -588.78\n",
      "iteration: 55 mean total reward: -579.32\n",
      "iteration: 56 mean total reward: -521.54\n",
      "iteration: 57 mean total reward: -522.3\n",
      "iteration: 58 mean total reward: -512.0\n",
      "iteration: 59 mean total reward: -609.54\n",
      "iteration: 60 mean total reward: -563.04\n",
      "iteration: 61 mean total reward: -468.04\n",
      "iteration: 62 mean total reward: -490.5\n",
      "iteration: 63 mean total reward: -601.32\n",
      "iteration: 64 mean total reward: -415.5\n",
      "iteration: 65 mean total reward: -483.12\n",
      "iteration: 66 mean total reward: -489.94\n",
      "iteration: 67 mean total reward: -439.04\n",
      "iteration: 68 mean total reward: -540.92\n",
      "iteration: 69 mean total reward: -489.32\n",
      "iteration: 70 mean total reward: -507.68\n",
      "iteration: 71 mean total reward: -528.38\n",
      "iteration: 72 mean total reward: -528.66\n",
      "iteration: 73 mean total reward: -578.92\n",
      "iteration: 74 mean total reward: -568.84\n",
      "iteration: 75 mean total reward: -541.92\n",
      "iteration: 76 mean total reward: -567.86\n",
      "iteration: 77 mean total reward: -587.5\n",
      "iteration: 78 mean total reward: -498.04\n",
      "iteration: 79 mean total reward: -490.22\n",
      "iteration: 80 mean total reward: -525.18\n",
      "iteration: 81 mean total reward: -522.92\n",
      "iteration: 82 mean total reward: -402.82\n",
      "iteration: 83 mean total reward: -460.0\n",
      "iteration: 84 mean total reward: -467.24\n",
      "iteration: 85 mean total reward: -575.06\n",
      "iteration: 86 mean total reward: -512.22\n",
      "iteration: 87 mean total reward: -382.82\n",
      "iteration: 88 mean total reward: -534.34\n",
      "iteration: 89 mean total reward: -489.92\n",
      "iteration: 90 mean total reward: -525.18\n",
      "iteration: 91 mean total reward: -576.9\n",
      "iteration: 92 mean total reward: -488.56\n",
      "iteration: 93 mean total reward: -553.16\n",
      "iteration: 94 mean total reward: -552.9\n",
      "iteration: 95 mean total reward: -435.6\n",
      "iteration: 96 mean total reward: -595.46\n",
      "iteration: 97 mean total reward: -484.72\n",
      "iteration: 98 mean total reward: -480.22\n",
      "iteration: 99 mean total reward: -487.72\n",
      "total reward: -361\n",
      "\n",
      "---------\n",
      "1: Smoothing = laplace\n",
      "iteration: 0 mean total reward: -802.86\n",
      "iteration: 1 mean total reward: -740.5\n",
      "iteration: 2 mean total reward: -698.08\n",
      "iteration: 3 mean total reward: -702.8\n",
      "iteration: 4 mean total reward: -655.82\n",
      "iteration: 5 mean total reward: -640.36\n",
      "iteration: 6 mean total reward: -632.08\n",
      "iteration: 7 mean total reward: -589.76\n",
      "iteration: 8 mean total reward: -644.9\n",
      "iteration: 9 mean total reward: -537.6\n",
      "iteration: 10 mean total reward: -597.36\n",
      "iteration: 11 mean total reward: -593.24\n",
      "iteration: 12 mean total reward: -555.78\n",
      "iteration: 13 mean total reward: -596.42\n",
      "iteration: 14 mean total reward: -537.42\n",
      "iteration: 15 mean total reward: -532.12\n",
      "iteration: 16 mean total reward: -505.72\n",
      "iteration: 17 mean total reward: -547.3\n",
      "iteration: 18 mean total reward: -572.42\n",
      "iteration: 19 mean total reward: -549.06\n",
      "iteration: 20 mean total reward: -559.48\n",
      "iteration: 21 mean total reward: -488.06\n",
      "iteration: 22 mean total reward: -522.18\n",
      "iteration: 23 mean total reward: -513.62\n",
      "iteration: 24 mean total reward: -517.26\n",
      "iteration: 25 mean total reward: -533.6\n",
      "iteration: 26 mean total reward: -522.16\n",
      "iteration: 27 mean total reward: -508.92\n",
      "iteration: 28 mean total reward: -450.04\n",
      "iteration: 29 mean total reward: -567.18\n",
      "iteration: 30 mean total reward: -577.86\n",
      "iteration: 31 mean total reward: -571.4\n",
      "iteration: 32 mean total reward: -517.84\n",
      "iteration: 33 mean total reward: -558.56\n",
      "iteration: 34 mean total reward: -472.6\n",
      "iteration: 35 mean total reward: -503.34\n",
      "iteration: 36 mean total reward: -479.54\n",
      "iteration: 37 mean total reward: -571.08\n",
      "iteration: 38 mean total reward: -549.66\n",
      "iteration: 39 mean total reward: -513.34\n",
      "iteration: 40 mean total reward: -501.34\n",
      "iteration: 41 mean total reward: -454.92\n",
      "iteration: 42 mean total reward: -442.26\n",
      "iteration: 43 mean total reward: -450.7\n",
      "iteration: 44 mean total reward: -535.48\n",
      "iteration: 45 mean total reward: -477.18\n",
      "iteration: 46 mean total reward: -518.9\n",
      "iteration: 47 mean total reward: -472.54\n",
      "iteration: 48 mean total reward: -452.48\n",
      "iteration: 49 mean total reward: -466.2\n",
      "iteration: 50 mean total reward: -589.48\n",
      "iteration: 51 mean total reward: -501.5\n",
      "iteration: 52 mean total reward: -450.28\n",
      "iteration: 53 mean total reward: -495.12\n",
      "iteration: 54 mean total reward: -492.78\n",
      "iteration: 55 mean total reward: -488.12\n",
      "iteration: 56 mean total reward: -483.7\n",
      "iteration: 57 mean total reward: -474.32\n",
      "iteration: 58 mean total reward: -455.48\n",
      "iteration: 59 mean total reward: -456.48\n",
      "iteration: 60 mean total reward: -460.48\n",
      "iteration: 61 mean total reward: -493.02\n",
      "iteration: 62 mean total reward: -474.94\n",
      "iteration: 63 mean total reward: -402.3\n",
      "iteration: 64 mean total reward: -458.94\n",
      "iteration: 65 mean total reward: -436.22\n",
      "iteration: 66 mean total reward: -414.9\n",
      "iteration: 67 mean total reward: -482.56\n",
      "iteration: 68 mean total reward: -436.7\n",
      "iteration: 69 mean total reward: -471.02\n",
      "iteration: 70 mean total reward: -509.98\n",
      "iteration: 71 mean total reward: -457.0\n",
      "iteration: 72 mean total reward: -413.1\n",
      "iteration: 73 mean total reward: -379.76\n",
      "iteration: 74 mean total reward: -396.78\n",
      "iteration: 75 mean total reward: -451.84\n",
      "iteration: 76 mean total reward: -417.48\n",
      "iteration: 77 mean total reward: -489.14\n",
      "iteration: 78 mean total reward: -448.56\n",
      "iteration: 79 mean total reward: -509.28\n",
      "iteration: 80 mean total reward: -405.74\n",
      "iteration: 81 mean total reward: -449.8\n",
      "iteration: 82 mean total reward: -408.64\n",
      "iteration: 83 mean total reward: -400.76\n",
      "iteration: 84 mean total reward: -462.62\n",
      "iteration: 85 mean total reward: -431.18\n",
      "iteration: 86 mean total reward: -386.88\n",
      "iteration: 87 mean total reward: -475.86\n",
      "iteration: 88 mean total reward: -434.58\n",
      "iteration: 89 mean total reward: -417.08\n",
      "iteration: 90 mean total reward: -429.64\n",
      "iteration: 91 mean total reward: -447.78\n",
      "iteration: 92 mean total reward: -467.14\n",
      "iteration: 93 mean total reward: -398.06\n",
      "iteration: 94 mean total reward: -486.12\n",
      "iteration: 95 mean total reward: -473.96\n",
      "iteration: 96 mean total reward: -437.24\n",
      "iteration: 97 mean total reward: -407.0\n",
      "iteration: 98 mean total reward: -448.26\n",
      "iteration: 99 mean total reward: -419.34\n",
      "total reward: -154\n",
      "\n",
      "---------\n",
      "2: Smoothing = policy\n",
      "iteration: 0 mean total reward: -776.98\n",
      "iteration: 1 mean total reward: -764.86\n",
      "iteration: 2 mean total reward: -716.26\n",
      "iteration: 3 mean total reward: -728.3\n",
      "iteration: 4 mean total reward: -690.78\n",
      "iteration: 5 mean total reward: -681.42\n",
      "iteration: 6 mean total reward: -555.88\n",
      "iteration: 7 mean total reward: -508.3\n",
      "iteration: 8 mean total reward: -524.02\n",
      "iteration: 9 mean total reward: -449.42\n",
      "iteration: 10 mean total reward: -511.46\n",
      "iteration: 11 mean total reward: -460.44\n",
      "iteration: 12 mean total reward: -461.06\n",
      "iteration: 13 mean total reward: -415.7\n",
      "iteration: 14 mean total reward: -490.78\n",
      "iteration: 15 mean total reward: -476.54\n",
      "iteration: 16 mean total reward: -494.46\n",
      "iteration: 17 mean total reward: -390.78\n",
      "iteration: 18 mean total reward: -415.58\n",
      "iteration: 19 mean total reward: -431.18\n",
      "iteration: 20 mean total reward: -492.44\n",
      "iteration: 21 mean total reward: -397.18\n",
      "iteration: 22 mean total reward: -483.7\n",
      "iteration: 23 mean total reward: -526.88\n",
      "iteration: 24 mean total reward: -476.78\n",
      "iteration: 25 mean total reward: -506.22\n",
      "iteration: 26 mean total reward: -448.2\n",
      "iteration: 27 mean total reward: -508.28\n",
      "iteration: 28 mean total reward: -391.76\n",
      "iteration: 29 mean total reward: -377.04\n",
      "iteration: 30 mean total reward: -415.24\n",
      "iteration: 31 mean total reward: -542.42\n",
      "iteration: 32 mean total reward: -442.28\n",
      "iteration: 33 mean total reward: -425.36\n",
      "iteration: 34 mean total reward: -418.74\n",
      "iteration: 35 mean total reward: -422.68\n",
      "iteration: 36 mean total reward: -420.86\n",
      "iteration: 37 mean total reward: -463.2\n",
      "iteration: 38 mean total reward: -377.44\n",
      "iteration: 39 mean total reward: -483.62\n",
      "iteration: 40 mean total reward: -406.88\n",
      "iteration: 41 mean total reward: -447.72\n",
      "iteration: 42 mean total reward: -438.3\n",
      "iteration: 43 mean total reward: -448.4\n",
      "iteration: 44 mean total reward: -432.02\n",
      "iteration: 45 mean total reward: -362.22\n",
      "iteration: 46 mean total reward: -379.4\n",
      "iteration: 47 mean total reward: -432.0\n",
      "iteration: 48 mean total reward: -399.62\n",
      "iteration: 49 mean total reward: -436.34\n",
      "iteration: 50 mean total reward: -413.68\n",
      "iteration: 51 mean total reward: -460.3\n",
      "iteration: 52 mean total reward: -402.5\n",
      "iteration: 53 mean total reward: -347.62\n",
      "iteration: 54 mean total reward: -409.64\n",
      "iteration: 55 mean total reward: -399.52\n",
      "iteration: 56 mean total reward: -489.74\n",
      "iteration: 57 mean total reward: -392.18\n",
      "iteration: 58 mean total reward: -397.36\n",
      "iteration: 59 mean total reward: -473.96\n",
      "iteration: 60 mean total reward: -431.1\n",
      "iteration: 61 mean total reward: -355.88\n",
      "iteration: 62 mean total reward: -411.98\n",
      "iteration: 63 mean total reward: -356.6\n",
      "iteration: 64 mean total reward: -372.92\n",
      "iteration: 65 mean total reward: -350.14\n",
      "iteration: 66 mean total reward: -480.92\n",
      "iteration: 67 mean total reward: -509.84\n",
      "iteration: 68 mean total reward: -389.28\n",
      "iteration: 69 mean total reward: -391.54\n",
      "iteration: 70 mean total reward: -467.94\n",
      "iteration: 71 mean total reward: -454.86\n",
      "iteration: 72 mean total reward: -422.04\n",
      "iteration: 73 mean total reward: -461.72\n",
      "iteration: 74 mean total reward: -367.94\n",
      "iteration: 75 mean total reward: -474.68\n",
      "iteration: 76 mean total reward: -587.92\n",
      "iteration: 77 mean total reward: -435.32\n",
      "iteration: 78 mean total reward: -393.68\n",
      "iteration: 79 mean total reward: -281.1\n",
      "iteration: 80 mean total reward: -406.56\n",
      "iteration: 81 mean total reward: -390.76\n",
      "iteration: 82 mean total reward: -443.36\n",
      "iteration: 83 mean total reward: -441.54\n",
      "iteration: 84 mean total reward: -383.14\n",
      "iteration: 85 mean total reward: -505.12\n",
      "iteration: 86 mean total reward: -435.74\n",
      "iteration: 87 mean total reward: -445.36\n",
      "iteration: 88 mean total reward: -398.62\n",
      "iteration: 89 mean total reward: -505.96\n",
      "iteration: 90 mean total reward: -418.26\n",
      "iteration: 91 mean total reward: -390.56\n",
      "iteration: 92 mean total reward: -473.18\n",
      "iteration: 93 mean total reward: -376.48\n",
      "iteration: 94 mean total reward: -475.86\n",
      "iteration: 95 mean total reward: -397.86\n",
      "iteration: 96 mean total reward: -459.24\n",
      "iteration: 97 mean total reward: -418.56\n",
      "iteration: 98 mean total reward: -406.56\n",
      "iteration: 99 mean total reward: -406.28\n",
      "total reward: 9\n",
      "\n",
      "---------\n",
      "3: Smoothing = both\n",
      "iteration: 0 mean total reward: -760.04\n",
      "iteration: 1 mean total reward: -758.86\n",
      "iteration: 2 mean total reward: -761.7\n",
      "iteration: 3 mean total reward: -745.76\n",
      "iteration: 4 mean total reward: -682.68\n",
      "iteration: 5 mean total reward: -667.6\n",
      "iteration: 6 mean total reward: -657.24\n",
      "iteration: 7 mean total reward: -624.7\n",
      "iteration: 8 mean total reward: -594.26\n",
      "iteration: 9 mean total reward: -583.82\n",
      "iteration: 10 mean total reward: -615.24\n",
      "iteration: 11 mean total reward: -662.84\n",
      "iteration: 12 mean total reward: -531.28\n",
      "iteration: 13 mean total reward: -565.22\n",
      "iteration: 14 mean total reward: -511.26\n",
      "iteration: 15 mean total reward: -536.5\n",
      "iteration: 16 mean total reward: -577.62\n",
      "iteration: 17 mean total reward: -567.54\n",
      "iteration: 18 mean total reward: -627.7\n",
      "iteration: 19 mean total reward: -562.02\n",
      "iteration: 20 mean total reward: -529.9\n",
      "iteration: 21 mean total reward: -577.52\n",
      "iteration: 22 mean total reward: -376.76\n",
      "iteration: 23 mean total reward: -508.96\n",
      "iteration: 24 mean total reward: -435.5\n",
      "iteration: 25 mean total reward: -498.94\n",
      "iteration: 26 mean total reward: -501.88\n",
      "iteration: 27 mean total reward: -494.78\n",
      "iteration: 28 mean total reward: -472.9\n",
      "iteration: 29 mean total reward: -466.84\n",
      "iteration: 30 mean total reward: -528.8\n",
      "iteration: 31 mean total reward: -450.66\n",
      "iteration: 32 mean total reward: -474.44\n",
      "iteration: 33 mean total reward: -478.7\n",
      "iteration: 34 mean total reward: -510.12\n",
      "iteration: 35 mean total reward: -386.68\n",
      "iteration: 36 mean total reward: -443.74\n",
      "iteration: 37 mean total reward: -392.46\n",
      "iteration: 38 mean total reward: -500.8\n",
      "iteration: 39 mean total reward: -425.8\n",
      "iteration: 40 mean total reward: -402.78\n",
      "iteration: 41 mean total reward: -500.2\n",
      "iteration: 42 mean total reward: -412.28\n",
      "iteration: 43 mean total reward: -434.16\n",
      "iteration: 44 mean total reward: -468.78\n",
      "iteration: 45 mean total reward: -418.6\n",
      "iteration: 46 mean total reward: -465.54\n",
      "iteration: 47 mean total reward: -384.24\n",
      "iteration: 48 mean total reward: -426.56\n",
      "iteration: 49 mean total reward: -476.34\n",
      "iteration: 50 mean total reward: -458.46\n",
      "iteration: 51 mean total reward: -460.88\n",
      "iteration: 52 mean total reward: -343.4\n",
      "iteration: 53 mean total reward: -412.96\n",
      "iteration: 54 mean total reward: -374.78\n",
      "iteration: 55 mean total reward: -408.6\n",
      "iteration: 56 mean total reward: -407.98\n",
      "iteration: 57 mean total reward: -420.72\n",
      "iteration: 58 mean total reward: -425.9\n",
      "iteration: 59 mean total reward: -410.68\n",
      "iteration: 60 mean total reward: -391.4\n",
      "iteration: 61 mean total reward: -374.1\n",
      "iteration: 62 mean total reward: -381.9\n",
      "iteration: 63 mean total reward: -392.04\n",
      "iteration: 64 mean total reward: -362.28\n",
      "iteration: 65 mean total reward: -366.78\n",
      "iteration: 66 mean total reward: -337.4\n",
      "iteration: 67 mean total reward: -368.36\n",
      "iteration: 68 mean total reward: -396.04\n",
      "iteration: 69 mean total reward: -443.06\n",
      "iteration: 70 mean total reward: -363.32\n",
      "iteration: 71 mean total reward: -409.24\n",
      "iteration: 72 mean total reward: -373.02\n",
      "iteration: 73 mean total reward: -329.96\n",
      "iteration: 74 mean total reward: -318.82\n",
      "iteration: 75 mean total reward: -440.42\n",
      "iteration: 76 mean total reward: -351.5\n",
      "iteration: 77 mean total reward: -333.3\n",
      "iteration: 78 mean total reward: -370.74\n",
      "iteration: 79 mean total reward: -429.5\n",
      "iteration: 80 mean total reward: -357.94\n",
      "iteration: 81 mean total reward: -344.22\n",
      "iteration: 82 mean total reward: -407.64\n",
      "iteration: 83 mean total reward: -395.16\n",
      "iteration: 84 mean total reward: -390.88\n",
      "iteration: 85 mean total reward: -351.64\n",
      "iteration: 86 mean total reward: -379.22\n",
      "iteration: 87 mean total reward: -347.42\n",
      "iteration: 88 mean total reward: -397.8\n",
      "iteration: 89 mean total reward: -418.74\n",
      "iteration: 90 mean total reward: -399.28\n",
      "iteration: 91 mean total reward: -358.38\n",
      "iteration: 92 mean total reward: -396.78\n",
      "iteration: 93 mean total reward: -371.26\n",
      "iteration: 94 mean total reward: -433.36\n",
      "iteration: 95 mean total reward: -377.32\n",
      "iteration: 96 mean total reward: -359.06\n",
      "iteration: 97 mean total reward: -387.16\n",
      "iteration: 98 mean total reward: -347.98\n",
      "iteration: 99 mean total reward: -377.54\n",
      "total reward: -205\n"
     ]
    }
   ],
   "source": [
    "environment = Environment(iteration_n=100)\n",
    "smoothings = [None, \"laplace\", \"policy\", \"both\"]\n",
    "mean_rewards_per_smoothings = [None] * 4\n",
    "for i, smoothing in enumerate(smoothings):\n",
    "    env.reset()\n",
    "    print(f\"\\n---------\\n{i}: Smoothing = {smoothing}\")\n",
    "    agent = CrossEntropyAgent(smoothing=smoothing)\n",
    "    mean_total_reward = train_agent(environment, agent)\n",
    "    mean_rewards_per_smoothings[i] = mean_total_reward\n",
    "\n",
    "    trajectory = get_trajectory(env, agent, max_len=100, visualize=False)\n",
    "    print('total reward:', sum(trajectory['rewards']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeea9f1-ccb4-4d2d-a407-770e5c6e1ca4",
   "metadata": {},
   "source": [
    "### Подбор гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ed565393-3b79-453d-8721-1ea93654675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    q_param = trial.suggest_float('q_param', 0, 1, log=False)\n",
    "    trajectory_n = trial.suggest_int('trajectory_n', 10, 500)\n",
    "    smoothing = trial.suggest_categorical('smoothing', [None, \"laplace\", \"policy\", \"both\"])\n",
    "    laplace_lambda=trial.suggest_float('laplace_lambda', 0, 1, log=False)\n",
    "    policy_lambda=trial.suggest_float('policy_lambda', 0, 1, log=False)\n",
    "    \n",
    "\n",
    "    agent = CrossEntropyAgent(smoothing=smoothing, laplace_lambda=laplace_lambda, policy_lambda=policy_lambda)\n",
    "    environment = Environment(q_param=q_param, trajectory_n=trajectory_n)\n",
    "    \n",
    "    mean_reward = train_agent(environment, agent, verbose=False)\n",
    "    \n",
    "    return mean_reward[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "98736242-95c0-4478-ba5e-31943df4c96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-12 14:24:31,022] A new study created in memory with name: no-name-a2bf70a5-62ae-4744-8479-6acdfdbcbdce\n",
      "[I 2023-10-12 14:24:43,692] Trial 0 finished with value: 6.934782608695652 and parameters: {'q_param': 0.41892372441782144, 'trajectory_n': 138, 'smoothing': 'policy', 'laplace_lambda': 0.7755780490725028, 'policy_lambda': 0.21999652492336763}. Best is trial 0 with value: 6.934782608695652.\n",
      "[I 2023-10-12 14:25:09,200] Trial 1 finished with value: -173.95857988165682 and parameters: {'q_param': 0.3785105156529047, 'trajectory_n': 169, 'smoothing': 'laplace', 'laplace_lambda': 0.6507360468460001, 'policy_lambda': 0.2387468040290298}. Best is trial 0 with value: 6.934782608695652.\n",
      "[I 2023-10-12 14:25:31,200] Trial 2 finished with value: 7.3937360178970915 and parameters: {'q_param': 0.37250789161018794, 'trajectory_n': 447, 'smoothing': None, 'laplace_lambda': 0.8646139991968085, 'policy_lambda': 0.9118078004954452}. Best is trial 2 with value: 7.3937360178970915.\n",
      "[I 2023-10-12 14:25:37,899] Trial 3 finished with value: -365.8484848484849 and parameters: {'q_param': 0.8423716465226455, 'trajectory_n': 33, 'smoothing': None, 'laplace_lambda': 0.052356541654715816, 'policy_lambda': 0.12299365882110003}. Best is trial 2 with value: 7.3937360178970915.\n",
      "[I 2023-10-12 14:27:48,961] Trial 4 finished with value: -151.64383561643837 and parameters: {'q_param': 0.062229739142051654, 'trajectory_n': 438, 'smoothing': 'policy', 'laplace_lambda': 0.6621451438837859, 'policy_lambda': 0.6059056714562864}. Best is trial 2 with value: 7.3937360178970915.\n",
      "[I 2023-10-12 14:28:30,173] Trial 5 finished with value: -305.52 and parameters: {'q_param': 0.07733695015658526, 'trajectory_n': 150, 'smoothing': 'both', 'laplace_lambda': 0.3939400836483484, 'policy_lambda': 0.8089616838169444}. Best is trial 2 with value: 7.3937360178970915.\n",
      "[I 2023-10-12 14:28:58,733] Trial 6 finished with value: -114.65702479338843 and parameters: {'q_param': 0.47172724681863076, 'trajectory_n': 242, 'smoothing': 'laplace', 'laplace_lambda': 0.6122532781534785, 'policy_lambda': 0.21808756549031516}. Best is trial 2 with value: 7.3937360178970915.\n",
      "[I 2023-10-12 14:30:25,624] Trial 7 finished with value: -578.3453237410072 and parameters: {'q_param': 0.8978175362259442, 'trajectory_n': 278, 'smoothing': 'both', 'laplace_lambda': 0.4670157355490794, 'policy_lambda': 0.012778849552562144}. Best is trial 2 with value: 7.3937360178970915.\n",
      "[I 2023-10-12 14:30:59,681] Trial 8 finished with value: -130.83710407239818 and parameters: {'q_param': 0.614683411088889, 'trajectory_n': 221, 'smoothing': 'policy', 'laplace_lambda': 0.8045397484330175, 'policy_lambda': 0.9624623664301409}. Best is trial 2 with value: 7.3937360178970915.\n",
      "[I 2023-10-12 14:31:47,715] Trial 9 finished with value: -64.08496732026144 and parameters: {'q_param': 0.40656676322995056, 'trajectory_n': 306, 'smoothing': None, 'laplace_lambda': 0.8256276682265952, 'policy_lambda': 0.6828568497637583}. Best is trial 2 with value: 7.3937360178970915.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "Value:  7.3937360178970915\n",
      "Params: \n",
      "  q_param: 0.37250789161018794\n",
      "  trajectory_n: 447\n",
      "  smoothing: None\n",
      "  laplace_lambda: 0.8646139991968085\n",
      "  policy_lambda: 0.9118078004954452\n"
     ]
    }
   ],
   "source": [
    "sampler = TPESampler()#n_startup_trials=10\n",
    "pruner = MedianPruner()\n",
    "\n",
    "study = optuna.create_study(sampler=sampler, pruner=pruner, direction='maximize')\n",
    "\n",
    "study.optimize(objective, n_trials=10)  \n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"Value: \", trial.value)\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ada30281-0009-4769-bd44-1d99ae55b257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_128.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plot_optimization_history(study, target_name='Reward')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "50caa52a-727a-4beb-9fa6-1afd076e5bf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_130.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plot_parallel_coordinate(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf9ee64-8704-4a1b-9ccc-899a9172895e",
   "metadata": {},
   "source": [
    "Как следует из графиков и результатов выше, агент лучше всего показывает себя без сглаживания, при этом среди сглаживаний лучше показало себя сглаживание по стратегии."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0641d6b2-bc8b-4438-9cb6-3add76d621be",
   "metadata": {},
   "source": [
    "## 3. Модификация для стохастических сред\n",
    "Если среда стохастическая, то можно использовать следующий трюк:\n",
    "1. По стратегии $\\pi_n$ cделаем выборку из $M$ детерминированных стратегий $\\pi_{n,m}$\n",
    "2. По стратегиям $\\pi_{n,m}$ получим $K$ траекторий $\\tau_{m,k}$\n",
    "3. Вычислим $$  V_{\\pi_{n,m}} \\coloneqq \\frac{1}{K}\\sum_kG(\\tau_{m,k}) $$\n",
    "4. Выберем \"элитные\" траектории $T_n = \\{ \\tau_{m,k} : V_{\\pi_{n,m}} > \\gamma_q \\}$, где $\\gamma_q$ -- $q$-квантиль по выборке $V_{\\pi_{n,m}}$\n",
    "5. Дальше делается все то же самое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a1c18769-d76d-451b-b64f-f09e83d7e5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticEnvironment:\n",
    "    def __init__(self, env_name='Taxi-v3', q_param=0.9, iteration_n=100, trajectory_n=50, packet_n=100):\n",
    "        self.env_name = env_name\n",
    "        self.q_param = q_param\n",
    "        self.iteration_n = iteration_n\n",
    "        self.iteration_range = np.array(range(self.iteration_n))\n",
    "        self.trajectory_n = trajectory_n\n",
    "        self.packet_n = packet_n\n",
    "        \n",
    "def train_agent_in_stochastic_env(environment, agent, verbose=True):\n",
    "    env = gym.make(environment.env_name)\n",
    "    env.reset()\n",
    "    mean_total_reward_per_iteration = np.zeros(environment.iteration_n)\n",
    "    \n",
    "    for iteration in environment.iteration_range:\n",
    "        packets = [\n",
    "            [get_trajectory(env, agent) for _ in range(environment.trajectory_n)] \n",
    "            for _ in range(environment.packet_n)\n",
    "        ]\n",
    "    \n",
    "        mean_reward_per_packet = [\n",
    "            np.mean(\n",
    "                [np.sum(trajectory['rewards']) for trajectory in trajectories]\n",
    "            )\n",
    "            for trajectories in packets\n",
    "        ]\n",
    "        mean_total_reward_per_iteration[iteration] = np.mean(mean_reward_per_packet)\n",
    "        \n",
    "        quantile = np.quantile(mean_reward_per_packet, environment.q_param)\n",
    "        elite_trajectories = []\n",
    "        for i, packet in enumerate(packets):\n",
    "            if mean_reward_per_packet[i] > quantile:\n",
    "                elite_trajectories.extend(packet)\n",
    "    \n",
    "        agent.fit(elite_trajectories)\n",
    "    return mean_total_reward_per_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599348fd-5e53-446f-aa3d-7e30b8ac0fa2",
   "metadata": {},
   "source": [
    "### Создадим игру и обучим агента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e4516062-9d96-4859-87e2-1225ce06387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = StochasticEnvironment()\n",
    "agent = CrossEntropyAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "265a1c93-e508-4f02-8d3d-d2605a8e34cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 mean total reward: -782.1\n",
      "iteration: 1 mean total reward: -750.26\n",
      "iteration: 2 mean total reward: -688.9\n",
      "iteration: 3 mean total reward: -698.56\n",
      "iteration: 4 mean total reward: -673.62\n",
      "iteration: 5 mean total reward: -555.26\n",
      "iteration: 6 mean total reward: -553.84\n",
      "iteration: 7 mean total reward: -558.64\n",
      "iteration: 8 mean total reward: -495.3\n",
      "iteration: 9 mean total reward: -556.7\n",
      "iteration: 10 mean total reward: -459.18\n",
      "iteration: 11 mean total reward: -571.78\n",
      "iteration: 12 mean total reward: -545.28\n",
      "iteration: 13 mean total reward: -484.32\n",
      "iteration: 14 mean total reward: -481.64\n",
      "iteration: 15 mean total reward: -605.12\n",
      "iteration: 16 mean total reward: -523.72\n",
      "iteration: 17 mean total reward: -435.8\n",
      "iteration: 18 mean total reward: -543.74\n",
      "iteration: 19 mean total reward: -528.94\n",
      "iteration: 20 mean total reward: -513.84\n",
      "iteration: 21 mean total reward: -534.22\n",
      "iteration: 22 mean total reward: -533.8\n",
      "iteration: 23 mean total reward: -548.96\n",
      "iteration: 24 mean total reward: -539.66\n",
      "iteration: 25 mean total reward: -433.36\n",
      "iteration: 26 mean total reward: -565.7\n",
      "iteration: 27 mean total reward: -458.46\n",
      "iteration: 28 mean total reward: -538.28\n",
      "iteration: 29 mean total reward: -376.14\n",
      "iteration: 30 mean total reward: -432.58\n",
      "iteration: 31 mean total reward: -398.0\n",
      "iteration: 32 mean total reward: -457.2\n",
      "iteration: 33 mean total reward: -488.52\n",
      "iteration: 34 mean total reward: -484.66\n",
      "iteration: 35 mean total reward: -498.06\n",
      "iteration: 36 mean total reward: -530.06\n",
      "iteration: 37 mean total reward: -527.68\n",
      "iteration: 38 mean total reward: -592.4\n",
      "iteration: 39 mean total reward: -415.04\n",
      "iteration: 40 mean total reward: -501.56\n",
      "iteration: 41 mean total reward: -534.66\n",
      "iteration: 42 mean total reward: -577.26\n",
      "iteration: 43 mean total reward: -449.54\n",
      "iteration: 44 mean total reward: -451.68\n",
      "iteration: 45 mean total reward: -517.76\n",
      "iteration: 46 mean total reward: -563.48\n",
      "iteration: 47 mean total reward: -553.84\n",
      "iteration: 48 mean total reward: -417.58\n",
      "iteration: 49 mean total reward: -523.04\n",
      "iteration: 50 mean total reward: -464.68\n",
      "iteration: 51 mean total reward: -558.82\n",
      "iteration: 52 mean total reward: -503.8\n",
      "iteration: 53 mean total reward: -455.84\n",
      "iteration: 54 mean total reward: -489.9\n",
      "iteration: 55 mean total reward: -528.92\n",
      "iteration: 56 mean total reward: -464.1\n",
      "iteration: 57 mean total reward: -497.4\n",
      "iteration: 58 mean total reward: -495.4\n",
      "iteration: 59 mean total reward: -481.68\n",
      "iteration: 60 mean total reward: -492.02\n",
      "iteration: 61 mean total reward: -463.46\n",
      "iteration: 62 mean total reward: -372.14\n",
      "iteration: 63 mean total reward: -507.54\n",
      "iteration: 64 mean total reward: -470.46\n",
      "iteration: 65 mean total reward: -544.52\n",
      "iteration: 66 mean total reward: -503.36\n",
      "iteration: 67 mean total reward: -453.3\n",
      "iteration: 68 mean total reward: -543.56\n",
      "iteration: 69 mean total reward: -348.46\n",
      "iteration: 70 mean total reward: -488.96\n",
      "iteration: 71 mean total reward: -534.44\n",
      "iteration: 72 mean total reward: -593.62\n",
      "iteration: 73 mean total reward: -581.58\n",
      "iteration: 74 mean total reward: -440.98\n",
      "iteration: 75 mean total reward: -439.2\n",
      "iteration: 76 mean total reward: -554.72\n",
      "iteration: 77 mean total reward: -532.56\n",
      "iteration: 78 mean total reward: -469.04\n",
      "iteration: 79 mean total reward: -532.96\n",
      "iteration: 80 mean total reward: -458.0\n",
      "iteration: 81 mean total reward: -517.02\n",
      "iteration: 82 mean total reward: -593.26\n",
      "iteration: 83 mean total reward: -494.68\n",
      "iteration: 84 mean total reward: -550.44\n",
      "iteration: 85 mean total reward: -531.4\n",
      "iteration: 86 mean total reward: -460.94\n",
      "iteration: 87 mean total reward: -474.82\n",
      "iteration: 88 mean total reward: -524.22\n",
      "iteration: 89 mean total reward: -498.5\n",
      "iteration: 90 mean total reward: -580.78\n",
      "iteration: 91 mean total reward: -498.0\n",
      "iteration: 92 mean total reward: -489.48\n",
      "iteration: 93 mean total reward: -519.94\n",
      "iteration: 94 mean total reward: -536.46\n",
      "iteration: 95 mean total reward: -500.68\n",
      "iteration: 96 mean total reward: -555.7\n",
      "iteration: 97 mean total reward: -487.14\n",
      "iteration: 98 mean total reward: -445.46\n",
      "iteration: 99 mean total reward: -514.74\n",
      "total reward: -442\n",
      "model:\n",
      "[[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.15384615 0.30769231 0.23076923 0.07692308 0.07692308 0.15384615]\n",
      " [0.         0.         0.         0.         1.         0.        ]\n",
      " ...\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.         1.         0.         0.         0.         0.        ]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]]\n"
     ]
    }
   ],
   "source": [
    "mean_total_reward_per_iteration = train_agent(environment, agent)\n",
    "\n",
    "trajectory = get_trajectory(env, agent, max_len=100, visualize=False)\n",
    "env.close()\n",
    "print('total reward:', sum(trajectory['rewards']))\n",
    "print('model:')\n",
    "print(agent.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5bbff66a-a5b1-4690-bb32-b0dfe8417139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_155.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=environment.iteration_range, y=mean_total_reward_per_iteration,\n",
    "                    mode='lines+markers'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Cross-Entropy Agent in Stochastic Environment: q={environment.q_param}, trajectory_n={environment.trajectory_n}, packet_n={environment.packet_n}\",\n",
    "    xaxis_title=\"Iteration\",\n",
    "    yaxis_title=\"Mean Total Reward\",\n",
    "    legend_title=\"Legend Title\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cae634-7b76-4bb7-9b60-8bdc2fffda47",
   "metadata": {},
   "source": [
    "Как следует из графика выше, результаты вышли хуже, чем когда рассматривалась детерминированная среда."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
